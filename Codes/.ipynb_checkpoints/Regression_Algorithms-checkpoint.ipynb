{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Regression Analysis Guide with scikit-learn\n", "\n", "Regression analysis is a statistical method used to model the relationship between a dependent (target) variable and one or more independent (predictor) variables. The goal is to predict the value of the target variable based on the values of the predictor variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Simple Linear Regression \ud83d\udcc8\n", "\n", "**Simple Linear Regression** is used to model the relationship between two continuous variables. It assumes a linear relationship, meaning it tries to fit a straight line ($y = mx + c$) to the data that best represents the observations.\n", "\n", "* **$y$**: Dependent variable\n", "* **$x$**: Independent variable\n", "* **$m$**: Slope of the line\n", "* **$c$**: Y-intercept"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "\n", "# Generate sample data\n", "np.random.seed(42)\n", "X = 2 * np.random.rand(100, 1)\n", "y = 4 + 3 * X + np.random.randn(100, 1)\n", "\n", "# Train-test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "# Model training\n", "lin_reg = LinearRegression()\n", "lin_reg.fit(X_train, y_train)\n", "\n", "print(f\"Intercept (c): {lin_reg.intercept_[0]}\")\n", "print(f\"Coefficient (m): {lin_reg.coef_[0][0]}\")\n", "\n", "# Predictions\n", "y_pred = lin_reg.predict(X_test)\n", "\n", "# Evaluation\n", "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n", "print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n", "\n", "# Visualization\n", "plt.scatter(X_test, y_test, color='blue', label='Actual')\n", "plt.plot(X_test, y_pred, color='red', label='Regression line')\n", "plt.title('Simple Linear Regression')\n", "plt.xlabel('X')\n", "plt.ylabel('y')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Polynomial Regression \ud83c\udfa2\n", "\n", "**Polynomial Regression** is used when the relationship between variables is non-linear."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import PolynomialFeatures\n", "\n", "np.random.seed(0)\n", "X = 2 - 3 * np.random.normal(0, 1, 100)\n", "y = X - 2 * (X ** 2) + 0.5 * (X ** 3) + np.random.normal(-3, 3, 100)\n", "X = X[:, np.newaxis]\n", "y = y[:, np.newaxis]\n", "\n", "degree = 3\n", "poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n", "X_poly = poly_features.fit_transform(X)\n", "\n", "poly_reg = LinearRegression()\n", "poly_reg.fit(X_poly, y)\n", "\n", "y_poly_pred = poly_reg.predict(X_poly)\n", "print(f\"R-squared: {r2_score(y, y_poly_pred)}\")\n", "\n", "X_sorted, y_poly_pred_sorted = zip(*sorted(zip(X, y_poly_pred)))\n", "\n", "plt.scatter(X, y, color='blue')\n", "plt.plot(X_sorted, y_poly_pred_sorted, color='red')\n", "plt.title(f'Polynomial Regression (Degree {degree})')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Ridge & Lasso Regression (Regularization) \u2696\ufe0f"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_california_housing\n", "from sklearn.linear_model import Ridge, Lasso\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "housing = fetch_california_housing()\n", "X, y = housing.data, housing.target\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "ridge_reg = Ridge(alpha=1.0)\n", "ridge_reg.fit(X_train_scaled, y_train)\n", "print(f\"Ridge R2: {r2_score(y_test, ridge_reg.predict(X_test_scaled))}\")\n", "\n", "lasso_reg = Lasso(alpha=0.1)\n", "lasso_reg.fit(X_train_scaled, y_train)\n", "print(f\"Lasso R2: {r2_score(y_test, lasso_reg.predict(X_test_scaled))}\")\n", "print(f\"Number of zero coeffs in Lasso: {np.sum(lasso_reg.coef_ == 0)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4. Logistic Regression \u2705/\u274c"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n", "\n", "iris = load_iris()\n", "X = iris.data[:100, :2]\n", "y = iris.target[:100]\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "log_reg = LogisticRegression()\n", "log_reg.fit(X_train, y_train)\n", "\n", "y_pred = log_reg.predict(X_test)\n", "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n", "\n", "h = .02\n", "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n", "\n", "Z = log_reg.predict(np.c_[xx.ravel(), yy.ravel()])\n", "Z = Z.reshape(xx.shape)\n", "plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.5)\n", "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdYlBu, edgecolor='k')\n", "plt.show()\n", "\n", "cm = confusion_matrix(y_test, y_pred)\n", "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names[:2])\n", "disp.plot()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}